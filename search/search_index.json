{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"simple-action-pipeline (sap) A simple pipeline framework for implementing sequential, parallel and dependant actions. Designed around two yaml configuration files and implementing an uncomplicated workflow manager. The simple action pipeline uses the python package \" Jug \" as it's underlying workflow manager and is provided as a pip installable python3 package. The simple action pipeline has some standard features which wrap around any application specific configuration or code. Using this documentation Most of these docs are primarily aimed at users of the simple-action-pipeline python package. Providing guidance on how install and create your own configurations, to create one or more bespoke pipelines. These docs can also be useful if you wish to extend or contribute towards this package.","title":"Home"},{"location":"#simple-action-pipeline-sap","text":"A simple pipeline framework for implementing sequential, parallel and dependant actions. Designed around two yaml configuration files and implementing an uncomplicated workflow manager. The simple action pipeline uses the python package \" Jug \" as it's underlying workflow manager and is provided as a pip installable python3 package. The simple action pipeline has some standard features which wrap around any application specific configuration or code.","title":"simple-action-pipeline (sap)"},{"location":"#using-this-documentation","text":"Most of these docs are primarily aimed at users of the simple-action-pipeline python package. Providing guidance on how install and create your own configurations, to create one or more bespoke pipelines. These docs can also be useful if you wish to extend or contribute towards this package.","title":"Using this documentation"},{"location":"development/","text":"Development Depends on: Python >=3.9 Create and activate a python virtual environment of your choice. Inside the virtual environment or machine: Install an editable version of simple-action-pipeline. git clone https://github.com/antarctica/simple-action-pipeline ./simple-action-pipeline cd ./simple-action-pipeline pip install -e . Use pip install -e \".[documentation]\" to edit/contribute to the documentation. Release/Versioning Version numbers should be used in tagging commits on the main branch and should be of the form v0.1.7 using the semantic versioning convention. Bugs & issues If you find a bug, or would like to contribute please raise an issue in the first instance. Building & deploying the documentation Run mkdocs build to build the docs. Then run mkdocs gh-deploy to deploy to the gh-pages branch of the repository. You must have write access to the repo.","title":"Development"},{"location":"development/#development","text":"Depends on: Python >=3.9 Create and activate a python virtual environment of your choice. Inside the virtual environment or machine: Install an editable version of simple-action-pipeline. git clone https://github.com/antarctica/simple-action-pipeline ./simple-action-pipeline cd ./simple-action-pipeline pip install -e . Use pip install -e \".[documentation]\" to edit/contribute to the documentation.","title":"Development"},{"location":"development/#releaseversioning","text":"Version numbers should be used in tagging commits on the main branch and should be of the form v0.1.7 using the semantic versioning convention.","title":"Release/Versioning"},{"location":"development/#bugs-issues","text":"If you find a bug, or would like to contribute please raise an issue in the first instance.","title":"Bugs &amp; issues"},{"location":"development/#building-deploying-the-documentation","text":"Run mkdocs build to build the docs. Then run mkdocs gh-deploy to deploy to the gh-pages branch of the repository. You must have write access to the repo.","title":"Building &amp; deploying the documentation"},{"location":"example/","text":"Example Basic tutorial Lets create a pipeline from scratch using the simple-action-pipeline package. Create a project directory for the pipeline mkdir hello-world-project Create and link to python virtual env assuming you have python >= 3.9 available cd hello-world-project python -m venv ./hello-venv ln -s ./hello-venv/bin/activate activate source activate python -m pip install sap@git+https://github.com/antarctica/simple-action-pipeline Create a pipeline directory mkdir hello-pipeline Create the required pipeline and application yamls cd hello-pipeline touch pipeline.yaml touch application.yaml Create the scripts directory and scripts mkdir hello-world-scripts touch hello-world-scripts/hello.sh touch hello-world-scripts/world.py Your project should now have this structure: hello-world-project/ \u2502 \u251c\u2500\u2500 hello-pipeline/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 hello-world-scripts/ \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 hello.sh \u2502 \u2502 \u2514\u2500\u2500 world.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 application.yaml \u2502 \u2514\u2500\u2500 pipeline.yaml \u2502 \u251c\u2500\u2500 activate \u2502 \u2514\u2500\u2500 hello-venv/ \u2502 \u251c\u2500\u2500 bin/ \u2514\u2500\u2500 ... Create the pipeline configuration Make the pipeline.yaml file contain the following: --- pipeline: name: tutorial description: pipeline configuration file for the tutorial pipeline env: description: environment variables for the pipeline create-env-file: true env-filename: pipeline.env variables: - PIPELINE_DIRECTORY: \"./\" - PIPELINE_MAXWORKERS: 1 Create the application configuration Make the application.yaml file contain the following: --- application: name: hello-world description: application configuration file for the hello-world tutorial env: description: environment variables for the application create-env-file: true env-filename: application.env variables: - PIPELINE_DIRECTORY: \"./\" - SCRIPTS_DIRECTORY: \"./hello-world-scripts\" - LETTER_DELAY_SECONDS: 2 sequence: description: sequence of actions for the application sequence: - script: name: hello.sh depends: '' - script: name: world.py depends: hello.sh Create both application scripts hello.sh #!/usr/bin/bash # hello.sh set -e string='hello ' delay=${LETTER_DELAY_SECONDS} for ((i=0; i<${#string}; i++)); do sleep ${delay} echo \"${string:i:1}\" done world.py from os import environ from time import sleep # world.py string = 'world' delay = int(environ['LETTER_DELAY_SECONDS']) for i in string: sleep(delay) print(i , end='\\n') Build the pipeline Go back to the root project directory. cd .. Build the pipeline. pipeline build ./hello-pipeline This should build without any errors. Check the pipeline status pipeline status ./hello-pipeline ... or try the shorter version. pipeline status ./hello-pipeline --short Run the pipeline pipeline execute ./hello-pipeline You can observe the script outputting to the terminal with 2 second gaps between each character. There will also be some output from the pipeline to show when tasks start and finish. At successful pipeline completion, a pipeline report is also output. (hello-venv) bash-4.2$ pipeline execute ./hello-pipeline INFO:pipeline:Target: ~/hello-world-project/hello-pipeline/ INFO:pipeline:Pipeline Status: built INFO:pipeline:Executing pipeline tutorial-hello-world.py ; MaxWorkers=1 (hello-venv) bash-4.2$ INFO:2025-03-25 08:44:50.224135 | started pipeline Failed Waiting Ready Complete Active Task name ------------------------------------------------------------------------------------ 0 0 1 0 0 tutorial-hello-world._00_start_pipeline 0 1 0 0 0 tutorial-hello-world._01_hello_sh 0 1 0 0 0 tutorial-hello-world._02_world_py 0 1 0 0 0 tutorial-hello-world._03_finish_pipeline .................................................................................... 0 3 1 0 0 Total INFO:2025-03-25 08:44:50.648218:pipeline:Task: _01_hello_sh started. h e l l o INFO:2025-03-25 08:45:02.678676:pipeline:Task: _01_hello_sh finished. INFO:2025-03-25 08:45:02.847103:pipeline:Task: _02_world_py started. w o r l d INFO:2025-03-25 08:45:12.894025:pipeline:Task: _02_world_py finished. INFO:2025-03-25 08:45:13.593772 | finished pipeline Failed Waiting Ready Complete Active Task name ------------------------------------------------------------------------------------ _00_start_pipeline 0 1 0 0 0 tutorial-hello-world._01_hello_sh 0 1 0 0 0 tutorial-hello-world._02_world_py 0 1 0 0 0 tutorial-hello-world._03_finish_pipeline .................................................................................... 0 3 1 0 0 Total Executed Loaded Task name ------------------------------------------------------------------------------------ 1 0 tutorial-hello-world._00_start_pipeline 1 0 tutorial-hello-world._01_hello_sh 1 0 tutorial-hello-world._02_world_py 1 0 tutorial-hello-world._03_finish_pipeline .................................................................................... 4 0 Total Make a change to the pipeline For this example lets make both scripts run much slower by increasing the LETTER_DELAY_SECONDS environment variable to 10 seconds. Change the definition in application.yaml and save it: LETTER_DELAY_SECONDS: 10 Rebuild the pipeline: pipeline build ./hello-pipeline --force-build Run the pipeline: pipeline execute ./hello-pipeline Now the pipeline tasks are slow enough that you can use the status command to check the pipeline whilst it runs: pipeline status ./hello-pipeline You can also halt the pipeline mid run: pipeline halt ./hello-pipeline Fully complete scripts will remain complete after a halt and from there you can resume the pipeline with the execute command: pipeline execute ./hello-pipeline Or you can choose to run the pipeline from the very beginning by issuing a reset command first: pipeline halt ./hello-pipeline pipeline reset ./hello-pipeline pipeline execute ./hello-pipeline Now try adding an extra script to the pipeline The sequence section of application.yaml allows you to create simple-linear or complex-parallel pipelines by defining more scripts with intricate dependancies. Remember, if you want multiple scripts to run in parallel you will need to increase the number of PIPELINE_MAXWORKERS in pipeline.yaml . Pre-made example There is a more complicated computational example pipeline in the GitHub repository here in the example directory. Using the commands outlined in Usage you can try out this example as the configuration files and application scripts are already made for you.","title":"Example"},{"location":"example/#example","text":"","title":"Example"},{"location":"example/#basic-tutorial","text":"Lets create a pipeline from scratch using the simple-action-pipeline package.","title":"Basic tutorial"},{"location":"example/#create-a-project-directory-for-the-pipeline","text":"mkdir hello-world-project","title":"Create a project directory for the pipeline"},{"location":"example/#create-and-link-to-python-virtual-env","text":"assuming you have python >= 3.9 available cd hello-world-project python -m venv ./hello-venv ln -s ./hello-venv/bin/activate activate source activate python -m pip install sap@git+https://github.com/antarctica/simple-action-pipeline","title":"Create and link to python virtual env"},{"location":"example/#create-a-pipeline-directory","text":"mkdir hello-pipeline","title":"Create a pipeline directory"},{"location":"example/#create-the-required-pipeline-and-application-yamls","text":"cd hello-pipeline touch pipeline.yaml touch application.yaml","title":"Create the required pipeline and application yamls"},{"location":"example/#create-the-scripts-directory-and-scripts","text":"mkdir hello-world-scripts touch hello-world-scripts/hello.sh touch hello-world-scripts/world.py","title":"Create the scripts directory and scripts"},{"location":"example/#your-project-should-now-have-this-structure","text":"hello-world-project/ \u2502 \u251c\u2500\u2500 hello-pipeline/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 hello-world-scripts/ \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 hello.sh \u2502 \u2502 \u2514\u2500\u2500 world.py \u2502 \u2502 \u2502 \u251c\u2500\u2500 application.yaml \u2502 \u2514\u2500\u2500 pipeline.yaml \u2502 \u251c\u2500\u2500 activate \u2502 \u2514\u2500\u2500 hello-venv/ \u2502 \u251c\u2500\u2500 bin/ \u2514\u2500\u2500 ...","title":"Your project should now have this structure:"},{"location":"example/#create-the-pipeline-configuration","text":"Make the pipeline.yaml file contain the following: --- pipeline: name: tutorial description: pipeline configuration file for the tutorial pipeline env: description: environment variables for the pipeline create-env-file: true env-filename: pipeline.env variables: - PIPELINE_DIRECTORY: \"./\" - PIPELINE_MAXWORKERS: 1","title":"Create the pipeline configuration"},{"location":"example/#create-the-application-configuration","text":"Make the application.yaml file contain the following: --- application: name: hello-world description: application configuration file for the hello-world tutorial env: description: environment variables for the application create-env-file: true env-filename: application.env variables: - PIPELINE_DIRECTORY: \"./\" - SCRIPTS_DIRECTORY: \"./hello-world-scripts\" - LETTER_DELAY_SECONDS: 2 sequence: description: sequence of actions for the application sequence: - script: name: hello.sh depends: '' - script: name: world.py depends: hello.sh","title":"Create the application configuration"},{"location":"example/#create-both-application-scripts","text":"hello.sh #!/usr/bin/bash # hello.sh set -e string='hello ' delay=${LETTER_DELAY_SECONDS} for ((i=0; i<${#string}; i++)); do sleep ${delay} echo \"${string:i:1}\" done world.py from os import environ from time import sleep # world.py string = 'world' delay = int(environ['LETTER_DELAY_SECONDS']) for i in string: sleep(delay) print(i , end='\\n')","title":"Create both application scripts"},{"location":"example/#build-the-pipeline","text":"Go back to the root project directory. cd .. Build the pipeline. pipeline build ./hello-pipeline This should build without any errors.","title":"Build the pipeline"},{"location":"example/#check-the-pipeline-status","text":"pipeline status ./hello-pipeline ... or try the shorter version. pipeline status ./hello-pipeline --short","title":"Check the pipeline status"},{"location":"example/#run-the-pipeline","text":"pipeline execute ./hello-pipeline You can observe the script outputting to the terminal with 2 second gaps between each character. There will also be some output from the pipeline to show when tasks start and finish. At successful pipeline completion, a pipeline report is also output. (hello-venv) bash-4.2$ pipeline execute ./hello-pipeline INFO:pipeline:Target: ~/hello-world-project/hello-pipeline/ INFO:pipeline:Pipeline Status: built INFO:pipeline:Executing pipeline tutorial-hello-world.py ; MaxWorkers=1 (hello-venv) bash-4.2$ INFO:2025-03-25 08:44:50.224135 | started pipeline Failed Waiting Ready Complete Active Task name ------------------------------------------------------------------------------------ 0 0 1 0 0 tutorial-hello-world._00_start_pipeline 0 1 0 0 0 tutorial-hello-world._01_hello_sh 0 1 0 0 0 tutorial-hello-world._02_world_py 0 1 0 0 0 tutorial-hello-world._03_finish_pipeline .................................................................................... 0 3 1 0 0 Total INFO:2025-03-25 08:44:50.648218:pipeline:Task: _01_hello_sh started. h e l l o INFO:2025-03-25 08:45:02.678676:pipeline:Task: _01_hello_sh finished. INFO:2025-03-25 08:45:02.847103:pipeline:Task: _02_world_py started. w o r l d INFO:2025-03-25 08:45:12.894025:pipeline:Task: _02_world_py finished. INFO:2025-03-25 08:45:13.593772 | finished pipeline Failed Waiting Ready Complete Active Task name ------------------------------------------------------------------------------------ _00_start_pipeline 0 1 0 0 0 tutorial-hello-world._01_hello_sh 0 1 0 0 0 tutorial-hello-world._02_world_py 0 1 0 0 0 tutorial-hello-world._03_finish_pipeline .................................................................................... 0 3 1 0 0 Total Executed Loaded Task name ------------------------------------------------------------------------------------ 1 0 tutorial-hello-world._00_start_pipeline 1 0 tutorial-hello-world._01_hello_sh 1 0 tutorial-hello-world._02_world_py 1 0 tutorial-hello-world._03_finish_pipeline .................................................................................... 4 0 Total","title":"Run the pipeline"},{"location":"example/#make-a-change-to-the-pipeline","text":"For this example lets make both scripts run much slower by increasing the LETTER_DELAY_SECONDS environment variable to 10 seconds. Change the definition in application.yaml and save it: LETTER_DELAY_SECONDS: 10 Rebuild the pipeline: pipeline build ./hello-pipeline --force-build Run the pipeline: pipeline execute ./hello-pipeline Now the pipeline tasks are slow enough that you can use the status command to check the pipeline whilst it runs: pipeline status ./hello-pipeline You can also halt the pipeline mid run: pipeline halt ./hello-pipeline Fully complete scripts will remain complete after a halt and from there you can resume the pipeline with the execute command: pipeline execute ./hello-pipeline Or you can choose to run the pipeline from the very beginning by issuing a reset command first: pipeline halt ./hello-pipeline pipeline reset ./hello-pipeline pipeline execute ./hello-pipeline","title":"Make a change to the pipeline"},{"location":"example/#now-try-adding-an-extra-script-to-the-pipeline","text":"The sequence section of application.yaml allows you to create simple-linear or complex-parallel pipelines by defining more scripts with intricate dependancies. Remember, if you want multiple scripts to run in parallel you will need to increase the number of PIPELINE_MAXWORKERS in pipeline.yaml .","title":"Now try adding an extra script to the pipeline"},{"location":"example/#pre-made-example","text":"There is a more complicated computational example pipeline in the GitHub repository here in the example directory. Using the commands outlined in Usage you can try out this example as the configuration files and application scripts are already made for you.","title":"Pre-made example"},{"location":"how-sap-works/","text":"How simple-action-pipeline works simple-action-pipeline (sap) provides a generic command line tool for creating configuration based reproducible pipelines. Behind the scenes sap uses some features of the workflow manager Jug to both create and then manage the pipeline. Firstly, sap treats a single directory as a pipeline, containing all of the required configuration and code for the pipeline. This is referred to as the <pipeline-directory> or <target-directory> . Secondly, sap builds the pipeline from pipeline.yaml , application.yaml and a scripts subdirectory. Building the pipeline is done by invoking the 'build' action. Once built there are a number of other actions as detailed in the Usage section. Minimum configuration As a bare minimum the pipeline.yaml and application.yaml must contain the following as demonstrated below: pipeline.yaml pipeline: name: pipeline_name description: pipeline configuration file env: description: environment variables for the pipeline create-env-file: true env-filename: pipeline.env variables: - PIPELINE_DIRECTORY: \"./\" - PIPELINE_MAXWORKERS: 1 application.yaml application: name: application_name description: application configuration file env: description: environment variables for the application create-env-file: true env-filename: application.env variables: - PIPELINE_DIRECTORY: \"./\" - SCRIPTS_DIRECTORY: \"./scripts\" sequence: description: sequence of actions for the application sequence: - script: name: first_script.sh depends: '' As you can see, this pipeline runs just a single script which depends on no other scripts. The maximum number of workers is set to 1 (this is how many parallel scripts can run at any one time). The PIPELINE_DIRECTORY must be defined in both yaml files as these are checked during the build process. PIPELINE_MAXWORKERS and SCRIPTS_DIRECTORY must also be defined. Also, any scripts defined under the 'sequence' section must exist in the scripts directory for the build to succeed. The workflow manager The pipeline 'build' command creates or re-creates from the application.yaml , pipeline.yaml and scripts directory, a python script that is used by the Jug parallelisation package. Inspection of this auto-generated python script shows how all the dependancies are set up. sap invokes Jug with this python script for each WORKER up to PIPELINE_MAXWORKERS , creating one or more parallel processes that can complete multiple tasks whilst being monitored. This collection of python script, Jug and WORKERS is referred to as the 'workflow-manager'. Everything related to the workflow manager's operation is contained within the <pipeline-directory>/workflow-manager/ directory, which is created by the 'build' command. Optimising PIPELINE_MAXWORKERS This pipeline.yaml file contains the PIPELINE_MAXWORKERS definition. The workflow manager will attempt to allocate up to this many workers to the pipeline. It can be important to think carefully about setting the maximum number of workers as described in the example below. Example: | You have 10 tasks the could all execute in parallel. | You are using a platform that has 6 CPU threads. * If you set MAXWORKERS to 2 the workflow manager will invoke 2 workers, meaning that the 2 CPU threads can complete all 10 tasks twice as quickly as if there was only 1 worker (i.e. 1 task done at a time). * If you set MAXWORKERS to 10 the workflow manager will invoke 10 workers but because this is more than available CPU threads there will be a significant amount of CPU context switching to achieve the effect of 10 CPU threads running. This results in slower performance. * If you set MAXWORKERS to 5 the workflow manager will invoke 5 workers, meaning that the 5 CPU threads can complete all 10 tasks five times as quickly as if there was only 1 worker (i.e. 1 task done at a time). This would also avoid CPU context switch and also leave 1 CPU thread free for the underlying platform. Environment variables If your pipeline relies upon constants held within environment variables, these can be pre-defined under the env:variables: section of either yaml config file. sap will make sure these environment variables are available whenever the pipeline runs. Task sequence The sequence order and dependancies of the tasks (scripts) are defined under the sequence: section of the application.yaml config file. Each task (script) in the sequence has a name: and depends: field. The name is the name of the script to be found in the scripts directory. The depends can be either a single script name or a list of script names if there are multiple dependancies. If a script has no dependancy then the depends: field should contain an empty string '' . Currently shell scripts .sh and python scripts .py are the only supported task (script) names. Further detail For more detail on the inner workings of Jug or sap , please refer to the documentation for: - Jug - simple-action-pipeline repo","title":"How simple-action-pipeline works"},{"location":"how-sap-works/#how-simple-action-pipeline-works","text":"simple-action-pipeline (sap) provides a generic command line tool for creating configuration based reproducible pipelines. Behind the scenes sap uses some features of the workflow manager Jug to both create and then manage the pipeline. Firstly, sap treats a single directory as a pipeline, containing all of the required configuration and code for the pipeline. This is referred to as the <pipeline-directory> or <target-directory> . Secondly, sap builds the pipeline from pipeline.yaml , application.yaml and a scripts subdirectory. Building the pipeline is done by invoking the 'build' action. Once built there are a number of other actions as detailed in the Usage section.","title":"How simple-action-pipeline works"},{"location":"how-sap-works/#minimum-configuration","text":"As a bare minimum the pipeline.yaml and application.yaml must contain the following as demonstrated below: pipeline.yaml pipeline: name: pipeline_name description: pipeline configuration file env: description: environment variables for the pipeline create-env-file: true env-filename: pipeline.env variables: - PIPELINE_DIRECTORY: \"./\" - PIPELINE_MAXWORKERS: 1 application.yaml application: name: application_name description: application configuration file env: description: environment variables for the application create-env-file: true env-filename: application.env variables: - PIPELINE_DIRECTORY: \"./\" - SCRIPTS_DIRECTORY: \"./scripts\" sequence: description: sequence of actions for the application sequence: - script: name: first_script.sh depends: '' As you can see, this pipeline runs just a single script which depends on no other scripts. The maximum number of workers is set to 1 (this is how many parallel scripts can run at any one time). The PIPELINE_DIRECTORY must be defined in both yaml files as these are checked during the build process. PIPELINE_MAXWORKERS and SCRIPTS_DIRECTORY must also be defined. Also, any scripts defined under the 'sequence' section must exist in the scripts directory for the build to succeed.","title":"Minimum configuration"},{"location":"how-sap-works/#the-workflow-manager","text":"The pipeline 'build' command creates or re-creates from the application.yaml , pipeline.yaml and scripts directory, a python script that is used by the Jug parallelisation package. Inspection of this auto-generated python script shows how all the dependancies are set up. sap invokes Jug with this python script for each WORKER up to PIPELINE_MAXWORKERS , creating one or more parallel processes that can complete multiple tasks whilst being monitored. This collection of python script, Jug and WORKERS is referred to as the 'workflow-manager'. Everything related to the workflow manager's operation is contained within the <pipeline-directory>/workflow-manager/ directory, which is created by the 'build' command.","title":"The workflow manager"},{"location":"how-sap-works/#optimising-pipeline_maxworkers","text":"This pipeline.yaml file contains the PIPELINE_MAXWORKERS definition. The workflow manager will attempt to allocate up to this many workers to the pipeline. It can be important to think carefully about setting the maximum number of workers as described in the example below. Example: | You have 10 tasks the could all execute in parallel. | You are using a platform that has 6 CPU threads. * If you set MAXWORKERS to 2 the workflow manager will invoke 2 workers, meaning that the 2 CPU threads can complete all 10 tasks twice as quickly as if there was only 1 worker (i.e. 1 task done at a time). * If you set MAXWORKERS to 10 the workflow manager will invoke 10 workers but because this is more than available CPU threads there will be a significant amount of CPU context switching to achieve the effect of 10 CPU threads running. This results in slower performance. * If you set MAXWORKERS to 5 the workflow manager will invoke 5 workers, meaning that the 5 CPU threads can complete all 10 tasks five times as quickly as if there was only 1 worker (i.e. 1 task done at a time). This would also avoid CPU context switch and also leave 1 CPU thread free for the underlying platform.","title":"Optimising PIPELINE_MAXWORKERS"},{"location":"how-sap-works/#environment-variables","text":"If your pipeline relies upon constants held within environment variables, these can be pre-defined under the env:variables: section of either yaml config file. sap will make sure these environment variables are available whenever the pipeline runs.","title":"Environment variables"},{"location":"how-sap-works/#task-sequence","text":"The sequence order and dependancies of the tasks (scripts) are defined under the sequence: section of the application.yaml config file. Each task (script) in the sequence has a name: and depends: field. The name is the name of the script to be found in the scripts directory. The depends can be either a single script name or a list of script names if there are multiple dependancies. If a script has no dependancy then the depends: field should contain an empty string '' . Currently shell scripts .sh and python scripts .py are the only supported task (script) names.","title":"Task sequence"},{"location":"how-sap-works/#further-detail","text":"For more detail on the inner workings of Jug or sap , please refer to the documentation for: - Jug - simple-action-pipeline repo","title":"Further detail"},{"location":"implementation/","text":"Implementation simple-action-pipeline is a python package which can create and run configuration based pipelines. There are three core principles underpinning the package implementation: Sequential Any pipeline is constructed from one or more 'actions' to be executed in sequence. Executing the pipeline will run through that sequence once. Each 'action' is a user created script (either a shell script or python script). Dependant Any 'action' (script) can be configured to have zero, one or many other 'actions' on which it is dependant. This is strictly enforced by simple-action-pipeline in that no 'action' may start until all configured dependancy 'actions' have completed successfully. Stateful simple-action-pipeline uses a workflow manager to maintain statefullness of the entire pipeline and all configured 'actions' within it. This statefullness is held \"on-disk\" rather than \"in-memory\" which means that a pipeline's state can be queried before, during or after the pipeline is executed. Components of a pipeline Pipeline directory Contains everything required to configure, build and run a pipeline. The diagram below shows how a project could have multiple independant pipelines. Pipeline configuration pipeline.yaml A yaml configuration file for the pipeline. Application configuration application.yaml A yaml configuration file for the 'application'. Here the 'application' refers to what you want the pipelines to do and how you want to do it. Scripts directory Where to put the user written 'actions' (scripts) that form the pipeline. This is not strictly enforced and can be configured to be any directory, although for neatness it helps to be inside the pipeline directory. workflow-manager directory This directory is automatically generated when the pipeline is built for the first time. After that, this directory holds live stateful information about the pipeline. Layout and Commands Commands In the diagram above there are examples of the CLI commands supported by simple-action-pipeline . For detailed information on supported commands, see the Usage section.","title":"Implementation"},{"location":"implementation/#implementation","text":"simple-action-pipeline is a python package which can create and run configuration based pipelines. There are three core principles underpinning the package implementation: Sequential Any pipeline is constructed from one or more 'actions' to be executed in sequence. Executing the pipeline will run through that sequence once. Each 'action' is a user created script (either a shell script or python script). Dependant Any 'action' (script) can be configured to have zero, one or many other 'actions' on which it is dependant. This is strictly enforced by simple-action-pipeline in that no 'action' may start until all configured dependancy 'actions' have completed successfully. Stateful simple-action-pipeline uses a workflow manager to maintain statefullness of the entire pipeline and all configured 'actions' within it. This statefullness is held \"on-disk\" rather than \"in-memory\" which means that a pipeline's state can be queried before, during or after the pipeline is executed.","title":"Implementation"},{"location":"implementation/#components-of-a-pipeline","text":"Pipeline directory Contains everything required to configure, build and run a pipeline. The diagram below shows how a project could have multiple independant pipelines. Pipeline configuration pipeline.yaml A yaml configuration file for the pipeline. Application configuration application.yaml A yaml configuration file for the 'application'. Here the 'application' refers to what you want the pipelines to do and how you want to do it. Scripts directory Where to put the user written 'actions' (scripts) that form the pipeline. This is not strictly enforced and can be configured to be any directory, although for neatness it helps to be inside the pipeline directory. workflow-manager directory This directory is automatically generated when the pipeline is built for the first time. After that, this directory holds live stateful information about the pipeline.","title":"Components of a pipeline"},{"location":"implementation/#layout-and-commands","text":"","title":"Layout and Commands"},{"location":"implementation/#commands","text":"In the diagram above there are examples of the CLI commands supported by simple-action-pipeline . For detailed information on supported commands, see the Usage section.","title":"Commands"},{"location":"installation/","text":"Installation It is recommended to use a Python virtual environment to reduce the risk of any Python package conflicts. Create a Python virtual environment The Python version must be Python 3.9 or higher. Check the available Python with python --version If required, install or load a compatible python version. Your system administrator will be able to help with getting a compatible Python version. then python -m venv <path-to-venv> with a path of your choosing. Source the new newly created python venv source <path-to-venv>/bin/activate (Assuming you're using Bash or similar. Use the appropriate activate script within that folder depending on your shell) Install the simple-action-pipeline (sap) package If you simply want to use the simple-action-pipeline 'as-is'. python -m pip install sap@git+https://github.com/antarctica/simple-action-pipeline If you want to install an editable version of simple-action-pipeline. git clone https://github.com/antarctica/simple-action-pipeline ./simple-action-pipeline cd ./simple-action-pipeline pip install -e . Use pip install -e \".[documentation]\" to edit/contribute to the documentation.","title":"Installation"},{"location":"installation/#installation","text":"It is recommended to use a Python virtual environment to reduce the risk of any Python package conflicts. Create a Python virtual environment The Python version must be Python 3.9 or higher. Check the available Python with python --version If required, install or load a compatible python version. Your system administrator will be able to help with getting a compatible Python version. then python -m venv <path-to-venv> with a path of your choosing. Source the new newly created python venv source <path-to-venv>/bin/activate (Assuming you're using Bash or similar. Use the appropriate activate script within that folder depending on your shell) Install the simple-action-pipeline (sap) package If you simply want to use the simple-action-pipeline 'as-is'. python -m pip install sap@git+https://github.com/antarctica/simple-action-pipeline If you want to install an editable version of simple-action-pipeline. git clone https://github.com/antarctica/simple-action-pipeline ./simple-action-pipeline cd ./simple-action-pipeline pip install -e . Use pip install -e \".[documentation]\" to edit/contribute to the documentation.","title":"Installation"},{"location":"using/","text":"Usage The primary method for interacting with the simple-action-pipeline package is via it's command-line-interface (CLI), with the command pipeline . make sure your python virtual environment is activated before issuing any of the commands below source <path-to-virtual-environment>\\bin\\activate Getting help -h, --help pipeline -h # or pipeline --help > usage: pipeline [-h] [-d PIPEDIR] [-b] [-r] [-s] {build,status,execute,reset,halt,await} [pipeline_directory ...] > > perform action with simple-action-pipeline by supplying a pipeline directory > > positional arguments: > {build,status,execute,reset,halt,await} > Action for the pipeline to perform. options are 'build', 'status', 'execute', 'reset', 'halt', 'await'. > pipeline_directory Pipeline directory to use > > options: > -h, --help show this help message and exit > -d PIPEDIR, --directory PIPEDIR Pipeline directory > -b, --force-build Force building the pipeline that is already built. > -r, --force-reset Force reset if the pipeline is still active > -s, --short Output shortened information where supported The standard usage for the pipeline command-line tool is: pipeline [options] {action} [target_directory] Build the pipeline build To build the pipeline from the pipeline.yaml and application.yaml files. This command is also required if you have made configuration changes to an already built pipeline. pipeline build <path-to-pipeline-directory> If you attempt to rebuild an already built pipeline you will be given a confirmation prompt. To suppress this the -b, --force-build option is provided. pipeline build <path-to-pipeline-directory> --force-build Get the pipeline status status As well as checking the status, it can also be used to check that the pipeline is installed and setup correctly. pipeline status <path-to-pipeline-directory> # or for the shorter output pipeline status <path-to-pipeline-directory> --short A long (or short) report should be output. This status command can be run at any time and will give the 'live' state of the pipeline and it's tasks. The status of the pipeline is stateful and persistent, even after any execution is completed, which is useful for querying long after the pipeline has completed. This holds true if the pipeline fails for any reason. Execute the pipeline execute To start the pipeline. pipeline execute <path-to-pipeline-directory> Because the pipeline state persists after completion (or failure) it must be reset before it can be executed again. Trying to execute a completed pipeline will result in no execution as all the work has already been done. pipeline execute ./example INFO:pipeline:All Tasks complete, nothing to do INFO:pipeline:Use the [reset] action before re-executing a completed pipeline Reset the pipeline reset Because the statefulness of the pipeline persists even after completion, an additional step is required before the pipeline can be executed again. This is called a reset , and when initiated, the workflow manager erases the state of the pipeline ready for re-execution. pipeline reset <path-to-pipeline-directory> Resetting a running pipeline is not advised and may produce unpredictable behaviour (please refer to halt below). Halt all pipeline tasks halt If the pipeline needs to be halted whilst it is running, the halt command has been provided. pipeline halt <path-to-pipeline-directory> This does not erase the statefulness of the pipeline, so the status command can be used after halting has occured. Any pipeline tasks that have already completed will remain so, although any tasks which haven't fully completed will revert to being not started. Following a 'halt' there are two possible choices: execute will resume the pipeline from where it was halted. reset will reset the pipeline to it's un-executed state. Running the pipeline with SLURM await The action await is a blocking process which only returns if the pipeline completes or encounters a task failure. pipeline await ./example This is useful because some schedulers such as SLURM may be unaware that the pipeline is running parallel tasks in the background. By placing a pipeline await action at the end of a job script the job is held active until it is fully complete or fails. Please refer to the provided example script slurm_job_example.sh . Tips Using any of the pipeline commands does not require sourcing of the pipeline's pipeline.env and application.env files beforehand, this is automatically handled by the pipeline. Running the pipeline command from inside a pipeline directory does not require specifying the <path-to-pipeline-directory> argument. When this argument is missing, the pipeline assumes the use of the current working directory. For instance, if you are inside the pipeline directory, you can simply issue the command pipeline status to get the current status.","title":"Usage"},{"location":"using/#usage","text":"The primary method for interacting with the simple-action-pipeline package is via it's command-line-interface (CLI), with the command pipeline . make sure your python virtual environment is activated before issuing any of the commands below source <path-to-virtual-environment>\\bin\\activate","title":"Usage"},{"location":"using/#getting-help-h-help","text":"pipeline -h # or pipeline --help > usage: pipeline [-h] [-d PIPEDIR] [-b] [-r] [-s] {build,status,execute,reset,halt,await} [pipeline_directory ...] > > perform action with simple-action-pipeline by supplying a pipeline directory > > positional arguments: > {build,status,execute,reset,halt,await} > Action for the pipeline to perform. options are 'build', 'status', 'execute', 'reset', 'halt', 'await'. > pipeline_directory Pipeline directory to use > > options: > -h, --help show this help message and exit > -d PIPEDIR, --directory PIPEDIR Pipeline directory > -b, --force-build Force building the pipeline that is already built. > -r, --force-reset Force reset if the pipeline is still active > -s, --short Output shortened information where supported The standard usage for the pipeline command-line tool is: pipeline [options] {action} [target_directory]","title":"Getting help -h, --help"},{"location":"using/#build-the-pipeline-build","text":"To build the pipeline from the pipeline.yaml and application.yaml files. This command is also required if you have made configuration changes to an already built pipeline. pipeline build <path-to-pipeline-directory> If you attempt to rebuild an already built pipeline you will be given a confirmation prompt. To suppress this the -b, --force-build option is provided. pipeline build <path-to-pipeline-directory> --force-build","title":"Build the pipeline build"},{"location":"using/#get-the-pipeline-status-status","text":"As well as checking the status, it can also be used to check that the pipeline is installed and setup correctly. pipeline status <path-to-pipeline-directory> # or for the shorter output pipeline status <path-to-pipeline-directory> --short A long (or short) report should be output. This status command can be run at any time and will give the 'live' state of the pipeline and it's tasks. The status of the pipeline is stateful and persistent, even after any execution is completed, which is useful for querying long after the pipeline has completed. This holds true if the pipeline fails for any reason.","title":"Get the pipeline status status"},{"location":"using/#execute-the-pipeline-execute","text":"To start the pipeline. pipeline execute <path-to-pipeline-directory> Because the pipeline state persists after completion (or failure) it must be reset before it can be executed again. Trying to execute a completed pipeline will result in no execution as all the work has already been done. pipeline execute ./example INFO:pipeline:All Tasks complete, nothing to do INFO:pipeline:Use the [reset] action before re-executing a completed pipeline","title":"Execute the pipeline execute"},{"location":"using/#reset-the-pipeline-reset","text":"Because the statefulness of the pipeline persists even after completion, an additional step is required before the pipeline can be executed again. This is called a reset , and when initiated, the workflow manager erases the state of the pipeline ready for re-execution. pipeline reset <path-to-pipeline-directory> Resetting a running pipeline is not advised and may produce unpredictable behaviour (please refer to halt below).","title":"Reset the pipeline reset"},{"location":"using/#halt-all-pipeline-tasks-halt","text":"If the pipeline needs to be halted whilst it is running, the halt command has been provided. pipeline halt <path-to-pipeline-directory> This does not erase the statefulness of the pipeline, so the status command can be used after halting has occured. Any pipeline tasks that have already completed will remain so, although any tasks which haven't fully completed will revert to being not started. Following a 'halt' there are two possible choices: execute will resume the pipeline from where it was halted. reset will reset the pipeline to it's un-executed state.","title":"Halt all pipeline tasks halt"},{"location":"using/#running-the-pipeline-with-slurm-await","text":"The action await is a blocking process which only returns if the pipeline completes or encounters a task failure. pipeline await ./example This is useful because some schedulers such as SLURM may be unaware that the pipeline is running parallel tasks in the background. By placing a pipeline await action at the end of a job script the job is held active until it is fully complete or fails. Please refer to the provided example script slurm_job_example.sh .","title":"Running the pipeline with SLURM await"},{"location":"using/#tips","text":"Using any of the pipeline commands does not require sourcing of the pipeline's pipeline.env and application.env files beforehand, this is automatically handled by the pipeline. Running the pipeline command from inside a pipeline directory does not require specifying the <path-to-pipeline-directory> argument. When this argument is missing, the pipeline assumes the use of the current working directory. For instance, if you are inside the pipeline directory, you can simply issue the command pipeline status to get the current status.","title":"Tips"}]}